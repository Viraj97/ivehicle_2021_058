{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef836325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import spacy\n",
    "import string\n",
    "import gensim\n",
    "import operator\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e771afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sem.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de87133f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Buy/Sell</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Model</th>\n",
       "      <th>Type</th>\n",
       "      <th>Color</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I want to buy brandnew toyota corolla black ca...</td>\n",
       "      <td>buy</td>\n",
       "      <td>brandnew</td>\n",
       "      <td>toyota</td>\n",
       "      <td>corolla</td>\n",
       "      <td>car</td>\n",
       "      <td>black</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I want to buy brandnew bmw x1 white suv which ...</td>\n",
       "      <td>buy</td>\n",
       "      <td>brandnew</td>\n",
       "      <td>bmw</td>\n",
       "      <td>x1</td>\n",
       "      <td>suv</td>\n",
       "      <td>white</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I want to buy recondition benz c200 green car ...</td>\n",
       "      <td>buy</td>\n",
       "      <td>recondition</td>\n",
       "      <td>benz</td>\n",
       "      <td>c200</td>\n",
       "      <td>car</td>\n",
       "      <td>green</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I want to buy brandnew audi a1 ash car which m...</td>\n",
       "      <td>buy</td>\n",
       "      <td>brandnew</td>\n",
       "      <td>audi</td>\n",
       "      <td>a1</td>\n",
       "      <td>car</td>\n",
       "      <td>ash</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I want to sell brandnew bmw x3 black suv which...</td>\n",
       "      <td>sell</td>\n",
       "      <td>brandnew</td>\n",
       "      <td>bmw</td>\n",
       "      <td>x3</td>\n",
       "      <td>suv</td>\n",
       "      <td>black</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No                                           Sentence Buy/Sell  \\\n",
       "0   1  I want to buy brandnew toyota corolla black ca...      buy   \n",
       "1   2  I want to buy brandnew bmw x1 white suv which ...      buy   \n",
       "2   3  I want to buy recondition benz c200 green car ...      buy   \n",
       "3   4  I want to buy brandnew audi a1 ash car which m...      buy   \n",
       "4   5  I want to sell brandnew bmw x3 black suv which...     sell   \n",
       "\n",
       "     Condition   Brand    Model Type  Color  Year  \n",
       "0     brandnew  toyota  corolla  car  black  2008  \n",
       "1     brandnew     bmw       x1  suv  white  2007  \n",
       "2  recondition    benz     c200  car  green  2017  \n",
       "3     brandnew    audi       a1  car    ash  2007  \n",
       "4     brandnew     bmw       x3  suv  black  2007  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c3d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the dataset from the CSV and save it to 'data_text'\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('sem.csv', error_bad_lines=False);\n",
    "\n",
    "# We only need the Headlines text column from the data\n",
    "data_text = data[:300000][['Sentence']];\n",
    "\n",
    "data_text['index'] = data_text.index\n",
    "\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "541d2192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Get the total number of documents\n",
    "'''\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a8b2af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I want to buy brandnew toyota corolla black ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I want to buy brandnew bmw x1 white suv which ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I want to buy recondition benz c200 green car ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I want to buy brandnew audi a1 ash car which m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I want to sell brandnew bmw x3 black suv which...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>I want to sell recondition toyota KDH blue car...</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>I want to buy brandnew benz c180 ash car which...</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>I want to sell brandnew bmw x3 black suv which...</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>I want to buy brandnew audi a3 white car which...</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>I want to sell brandnew toyota yaris red car w...</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  index\n",
       "0    I want to buy brandnew toyota corolla black ca...      0\n",
       "1    I want to buy brandnew bmw x1 white suv which ...      1\n",
       "2    I want to buy recondition benz c200 green car ...      2\n",
       "3    I want to buy brandnew audi a1 ash car which m...      3\n",
       "4    I want to sell brandnew bmw x3 black suv which...      4\n",
       "..                                                 ...    ...\n",
       "495  I want to sell recondition toyota KDH blue car...    495\n",
       "496  I want to buy brandnew benz c180 ash car which...    496\n",
       "497  I want to sell brandnew bmw x3 black suv which...    497\n",
       "498  I want to buy brandnew audi a3 white car which...    498\n",
       "499  I want to sell brandnew toyota yaris red car w...    499\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c51e70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading Gensim and nltk libraries\n",
    "'''\n",
    "# !pip install gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcbf308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('bought', pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a881c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>want</td>\n",
       "      <td>want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buy</td>\n",
       "      <td>buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>corolla</td>\n",
       "      <td>corolla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>toyota</td>\n",
       "      <td>toyota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bmw</td>\n",
       "      <td>bmw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>audi</td>\n",
       "      <td>audi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>benz</td>\n",
       "      <td>benz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>black</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>green</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sell</td>\n",
       "      <td>sell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>KDH</td>\n",
       "      <td>kdh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2009</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2007</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2011</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2012</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2103</td>\n",
       "      <td>2103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2014</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2019</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2020</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>brandnew</td>\n",
       "      <td>brandnew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>recondition</td>\n",
       "      <td>recondit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>red</td>\n",
       "      <td>red</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>white</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>x1</td>\n",
       "      <td>x1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>c200</td>\n",
       "      <td>c200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>c180</td>\n",
       "      <td>c180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word   stemmed\n",
       "0           want      want\n",
       "1            buy       buy\n",
       "2        corolla   corolla\n",
       "3         toyota    toyota\n",
       "4            bmw       bmw\n",
       "5           audi      audi\n",
       "6           benz      benz\n",
       "7          black     black\n",
       "8          green     green\n",
       "9              i         i\n",
       "10          sell      sell\n",
       "11           KDH       kdh\n",
       "12          2008      2008\n",
       "13          2009      2009\n",
       "14          2007      2007\n",
       "15          2010      2010\n",
       "16          2011      2011\n",
       "17          2012      2012\n",
       "18          2103      2103\n",
       "19          2014      2014\n",
       "20          2015      2015\n",
       "21          2016      2016\n",
       "22          2017      2017\n",
       "23          2018      2018\n",
       "24          2019      2019\n",
       "25          2020      2020\n",
       "26          2021      2021\n",
       "27      brandnew  brandnew\n",
       "28   recondition  recondit\n",
       "29           red       red\n",
       "30         white     white\n",
       "31            x1        x1\n",
       "32          c200      c200\n",
       "33          c180      c180"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "original_words = ['want','buy','corolla','toyota','bmw','audi','benz','black','green','i','sell','KDH','2008','2009','2007','2010','2011','2012','2103','2014','2015','2016','2017','2018','2019','2020','2021','brandnew','recondition','red','white','x1','c200','c180']\n",
    "\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "\n",
    "pd.DataFrame(data={'original word':original_words, 'stemmed':singles })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb1548ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        \n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            \n",
    "            # TODO: Apply lemmatize_stemming() on the token, then add to the results list\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a688a7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['I', 'want', 'to', 'buy', 'brandnew', 'toyota', 'KDH', 'black', 'car', 'which', 'made', 'in', '2008']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['want', 'brandnew', 'toyota', 'black']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview a document after preprocessing\n",
    "'''\n",
    "document_num = 20\n",
    "doc_sample = documents[documents['index'] == document_num].values[0][0]\n",
    "\n",
    "print(\"Original document: \")\n",
    "\n",
    "words = []\n",
    "\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "    \n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "199cef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: preprocess all the headlines, saving the list of results as 'processed_docs'\n",
    "processed_docs = documents['Sentence'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9fd976b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [want, brandnew, toyota, corolla, black]\n",
       "1                       [want, brandnew, white]\n",
       "2                 [want, recondit, benz, green]\n",
       "3                        [want, brandnew, audi]\n",
       "4                 [want, sell, brandnew, black]\n",
       "                         ...                   \n",
       "495        [want, sell, recondit, toyota, blue]\n",
       "496                      [want, brandnew, benz]\n",
       "497               [want, sell, brandnew, black]\n",
       "498               [want, brandnew, audi, white]\n",
       "499        [want, sell, brandnew, toyota, yari]\n",
       "Name: Sentence, Length: 500, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Preview 'processed_docs'\n",
    "'''\n",
    "processed_docs[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a80e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Create a dictionary from 'processed_docs' containing the number of times a word appears \n",
    "in the training set using gensim.corpora.Dictionary and call it 'dictionary'\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe2f345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 black\n",
      "1 brandnew\n",
      "2 corolla\n",
      "3 toyota\n",
      "4 want\n",
      "5 white\n",
      "6 benz\n",
      "7 green\n",
      "8 recondit\n",
      "9 audi\n",
      "10 sell\n",
      "11 allion\n",
      "12 carb\n",
      "13 yari\n",
      "14 blue\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for k, v in dictionary.iteritems():\n",
    "    \n",
    "    print(k, v)\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count > 500:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23cab3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP\n",
    "Remove very rare and very common words:\n",
    "\n",
    "- words appearing less than 5 times\n",
    "- words appearing in more than 50% of all documents\n",
    "'''\n",
    "# TODO: apply dictionary.filter_extremes() with the parameters mentioned above\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9e6afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dff9a7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (2, 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[document_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fb6e797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"black\") appears 0 time.\n",
      "Word 2 (\"toyota\") appears 2 time.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview BOW for our sample preprocessed document\n",
    "'''\n",
    "# Here document_num is document number 4310 which we have checked in Step 2\n",
    "bow_doc_500 = bow_corpus[document_num]\n",
    "\n",
    "for i in range(len(bow_doc_500)):\n",
    "    \n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_500[i][0], \n",
    "                                                     dictionary[bow_doc_500[i][0]], \n",
    "                                                     bow_doc_500[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fb953bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=500, num_nnz=1201)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create tf-idf model object using models.TfidfModel on 'bow_corpus' and save it to 'tfidf'\n",
    "'''\n",
    "from gensim import corpora, models\n",
    "\n",
    "# TODO\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2528a0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Apply transformation to the entire corpus and call it 'corpus_tfidf'\n",
    "'''\n",
    "# TODO\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "print(corpus_tfidf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d447b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.44891263932132824), (1, 0.7726285553324562), (2, 0.44891263932132824)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview TF-IDF scores for our first document --> --> (token_id, tfidf score)\n",
    "'''\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    \n",
    "    pprint(doc)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bc06971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA mono-core -- fallback code in case LdaMulticore throws an error on your machine\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, \n",
    "#                                    num_topics = 10, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 50)\n",
    "\n",
    "# LDA multicore \n",
    "'''\n",
    "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
    "'''\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics=10, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75729157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.230*\"white\" + 0.191*\"sell\" + 0.176*\"recondit\" + 0.150*\"toyota\" + 0.148*\"allion\" + 0.058*\"audi\" + 0.033*\"green\" + 0.006*\"black\" + 0.003*\"yari\" + 0.003*\"carb\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.291*\"sell\" + 0.260*\"toyota\" + 0.206*\"yari\" + 0.057*\"recondit\" + 0.030*\"benz\" + 0.030*\"corolla\" + 0.027*\"green\" + 0.024*\"audi\" + 0.024*\"black\" + 0.022*\"allion\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.395*\"recondit\" + 0.262*\"green\" + 0.161*\"benz\" + 0.121*\"audi\" + 0.025*\"sell\" + 0.011*\"toyota\" + 0.009*\"white\" + 0.007*\"corolla\" + 0.005*\"black\" + 0.003*\"yari\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.241*\"sell\" + 0.234*\"toyota\" + 0.224*\"recondit\" + 0.211*\"corolla\" + 0.018*\"audi\" + 0.018*\"green\" + 0.011*\"black\" + 0.011*\"carb\" + 0.011*\"yari\" + 0.008*\"white\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.530*\"white\" + 0.182*\"audi\" + 0.139*\"benz\" + 0.129*\"recondit\" + 0.010*\"black\" + 0.003*\"toyota\" + 0.003*\"corolla\" + 0.001*\"sell\" + 0.001*\"green\" + 0.001*\"yari\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.469*\"audi\" + 0.199*\"sell\" + 0.197*\"green\" + 0.100*\"black\" + 0.016*\"recondit\" + 0.012*\"white\" + 0.001*\"benz\" + 0.001*\"toyota\" + 0.001*\"yari\" + 0.001*\"corolla\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.237*\"toyota\" + 0.220*\"yari\" + 0.195*\"carb\" + 0.104*\"recondit\" + 0.073*\"audi\" + 0.048*\"black\" + 0.043*\"sell\" + 0.035*\"white\" + 0.019*\"green\" + 0.012*\"benz\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.290*\"black\" + 0.286*\"toyota\" + 0.228*\"corolla\" + 0.051*\"sell\" + 0.049*\"benz\" + 0.028*\"white\" + 0.026*\"yari\" + 0.022*\"carb\" + 0.018*\"recondit\" + 0.001*\"audi\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.423*\"black\" + 0.321*\"sell\" + 0.115*\"audi\" + 0.049*\"white\" + 0.027*\"recondit\" + 0.024*\"toyota\" + 0.014*\"corolla\" + 0.009*\"allion\" + 0.006*\"benz\" + 0.006*\"green\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.420*\"benz\" + 0.169*\"black\" + 0.165*\"sell\" + 0.147*\"recondit\" + 0.028*\"white\" + 0.025*\"audi\" + 0.013*\"toyota\" + 0.013*\"yari\" + 0.013*\"green\" + 0.007*\"carb\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    \n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43e4477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define lda model using corpus_tfidf, again using gensim.models.LdaMulticore()\n",
    "'''\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                             num_topics=10, \n",
    "                                             id2word = dictionary, \n",
    "                                             passes = 2, \n",
    "                                             workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0f86ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.585*\"white\" + 0.153*\"benz\" + 0.060*\"audi\" + 0.048*\"black\" + 0.044*\"sell\" + 0.035*\"recondit\" + 0.025*\"yari\" + 0.021*\"toyota\" + 0.019*\"carb\" + 0.009*\"allion\"\n",
      "\n",
      "\n",
      "Topic: 1 Word: 0.191*\"white\" + 0.187*\"black\" + 0.167*\"audi\" + 0.084*\"recondit\" + 0.082*\"benz\" + 0.082*\"sell\" + 0.077*\"corolla\" + 0.067*\"toyota\" + 0.033*\"green\" + 0.020*\"allion\"\n",
      "\n",
      "\n",
      "Topic: 2 Word: 0.333*\"audi\" + 0.296*\"green\" + 0.152*\"recondit\" + 0.092*\"white\" + 0.084*\"sell\" + 0.008*\"carb\" + 0.008*\"toyota\" + 0.008*\"benz\" + 0.006*\"yari\" + 0.006*\"allion\"\n",
      "\n",
      "\n",
      "Topic: 3 Word: 0.347*\"black\" + 0.239*\"sell\" + 0.101*\"white\" + 0.086*\"audi\" + 0.059*\"benz\" + 0.052*\"recondit\" + 0.040*\"green\" + 0.027*\"toyota\" + 0.024*\"corolla\" + 0.020*\"allion\"\n",
      "\n",
      "\n",
      "Topic: 4 Word: 0.399*\"recondit\" + 0.261*\"benz\" + 0.160*\"green\" + 0.105*\"black\" + 0.020*\"white\" + 0.015*\"audi\" + 0.013*\"corolla\" + 0.011*\"toyota\" + 0.008*\"allion\" + 0.006*\"sell\"\n",
      "\n",
      "\n",
      "Topic: 5 Word: 0.345*\"benz\" + 0.186*\"sell\" + 0.166*\"allion\" + 0.094*\"toyota\" + 0.088*\"white\" + 0.068*\"recondit\" + 0.013*\"corolla\" + 0.012*\"yari\" + 0.011*\"black\" + 0.010*\"green\"\n",
      "\n",
      "\n",
      "Topic: 6 Word: 0.262*\"corolla\" + 0.238*\"black\" + 0.187*\"sell\" + 0.185*\"toyota\" + 0.068*\"recondit\" + 0.024*\"yari\" + 0.012*\"green\" + 0.011*\"audi\" + 0.006*\"allion\" + 0.005*\"carb\"\n",
      "\n",
      "\n",
      "Topic: 7 Word: 0.287*\"audi\" + 0.235*\"yari\" + 0.145*\"toyota\" + 0.119*\"black\" + 0.112*\"carb\" + 0.072*\"sell\" + 0.013*\"white\" + 0.006*\"recondit\" + 0.006*\"corolla\" + 0.003*\"allion\"\n",
      "\n",
      "\n",
      "Topic: 8 Word: 0.249*\"benz\" + 0.158*\"sell\" + 0.124*\"green\" + 0.097*\"yari\" + 0.085*\"carb\" + 0.083*\"audi\" + 0.066*\"toyota\" + 0.054*\"recondit\" + 0.043*\"black\" + 0.023*\"white\"\n",
      "\n",
      "\n",
      "Topic: 9 Word: 0.262*\"benz\" + 0.187*\"black\" + 0.150*\"sell\" + 0.122*\"white\" + 0.115*\"recondit\" + 0.050*\"audi\" + 0.035*\"toyota\" + 0.033*\"allion\" + 0.024*\"yari\" + 0.013*\"corolla\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    \n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ce1be06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['want', 'sell', 'recondit', 'toyota', 'blue']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Text of sample document 20\n",
    "'''\n",
    "processed_docs[495]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b5e0d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.819974422454834\t \n",
      "Topic: 0.241*\"sell\" + 0.234*\"toyota\" + 0.224*\"recondit\" + 0.211*\"corolla\" + 0.018*\"audi\" + 0.018*\"green\" + 0.011*\"black\" + 0.011*\"carb\" + 0.011*\"yari\" + 0.008*\"white\"\n",
      "\n",
      "Score: 0.02000446245074272\t \n",
      "Topic: 0.291*\"sell\" + 0.260*\"toyota\" + 0.206*\"yari\" + 0.057*\"recondit\" + 0.030*\"benz\" + 0.030*\"corolla\" + 0.027*\"green\" + 0.024*\"audi\" + 0.024*\"black\" + 0.022*\"allion\"\n",
      "\n",
      "Score: 0.020004216581583023\t \n",
      "Topic: 0.290*\"black\" + 0.286*\"toyota\" + 0.228*\"corolla\" + 0.051*\"sell\" + 0.049*\"benz\" + 0.028*\"white\" + 0.026*\"yari\" + 0.022*\"carb\" + 0.018*\"recondit\" + 0.001*\"audi\"\n",
      "\n",
      "Score: 0.0200036633759737\t \n",
      "Topic: 0.230*\"white\" + 0.191*\"sell\" + 0.176*\"recondit\" + 0.150*\"toyota\" + 0.148*\"allion\" + 0.058*\"audi\" + 0.033*\"green\" + 0.006*\"black\" + 0.003*\"yari\" + 0.003*\"carb\"\n",
      "\n",
      "Score: 0.020003195852041245\t \n",
      "Topic: 0.395*\"recondit\" + 0.262*\"green\" + 0.161*\"benz\" + 0.121*\"audi\" + 0.025*\"sell\" + 0.011*\"toyota\" + 0.009*\"white\" + 0.007*\"corolla\" + 0.005*\"black\" + 0.003*\"yari\"\n",
      "\n",
      "Score: 0.020002689212560654\t \n",
      "Topic: 0.237*\"toyota\" + 0.220*\"yari\" + 0.195*\"carb\" + 0.104*\"recondit\" + 0.073*\"audi\" + 0.048*\"black\" + 0.043*\"sell\" + 0.035*\"white\" + 0.019*\"green\" + 0.012*\"benz\"\n",
      "\n",
      "Score: 0.02000262588262558\t \n",
      "Topic: 0.423*\"black\" + 0.321*\"sell\" + 0.115*\"audi\" + 0.049*\"white\" + 0.027*\"recondit\" + 0.024*\"toyota\" + 0.014*\"corolla\" + 0.009*\"allion\" + 0.006*\"benz\" + 0.006*\"green\"\n",
      "\n",
      "Score: 0.020002298057079315\t \n",
      "Topic: 0.420*\"benz\" + 0.169*\"black\" + 0.165*\"sell\" + 0.147*\"recondit\" + 0.028*\"white\" + 0.025*\"audi\" + 0.013*\"toyota\" + 0.013*\"yari\" + 0.013*\"green\" + 0.007*\"carb\"\n",
      "\n",
      "Score: 0.020001446828246117\t \n",
      "Topic: 0.469*\"audi\" + 0.199*\"sell\" + 0.197*\"green\" + 0.100*\"black\" + 0.016*\"recondit\" + 0.012*\"white\" + 0.001*\"benz\" + 0.001*\"toyota\" + 0.001*\"yari\" + 0.001*\"corolla\"\n",
      "\n",
      "Score: 0.020000962540507317\t \n",
      "Topic: 0.530*\"white\" + 0.182*\"audi\" + 0.139*\"benz\" + 0.129*\"recondit\" + 0.010*\"black\" + 0.003*\"toyota\" + 0.003*\"corolla\" + 0.001*\"sell\" + 0.001*\"green\" + 0.001*\"yari\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check which topic our test document belongs to using the LDA Bag of Words model.\n",
    "'''\n",
    "document_num = 15\n",
    "# Our test document is document number 4310\n",
    "\n",
    "for index, score in sorted(lda_model[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf3922d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5984252691268921\t \n",
      "Topic: 0.343*\"corolla\" + 0.217*\"toyota\" + 0.109*\"sell\" + 0.109*\"recondit\" + 0.098*\"white\" + 0.025*\"black\" + 0.025*\"benz\" + 0.025*\"audi\" + 0.025*\"yari\" + 0.025*\"green\"\n",
      "\n",
      "Score: 0.24154122173786163\t \n",
      "Topic: 0.351*\"recondit\" + 0.226*\"white\" + 0.127*\"audi\" + 0.116*\"toyota\" + 0.092*\"sell\" + 0.018*\"benz\" + 0.018*\"black\" + 0.018*\"green\" + 0.018*\"yari\" + 0.018*\"corolla\"\n",
      "\n",
      "Score: 0.020018702372908592\t \n",
      "Topic: 0.457*\"black\" + 0.361*\"sell\" + 0.023*\"white\" + 0.023*\"benz\" + 0.023*\"audi\" + 0.023*\"toyota\" + 0.023*\"recondit\" + 0.023*\"corolla\" + 0.023*\"green\" + 0.023*\"yari\"\n",
      "\n",
      "Score: 0.02000558376312256\t \n",
      "Topic: 0.424*\"benz\" + 0.144*\"sell\" + 0.126*\"black\" + 0.100*\"recondit\" + 0.055*\"green\" + 0.054*\"white\" + 0.045*\"audi\" + 0.017*\"yari\" + 0.017*\"toyota\" + 0.017*\"corolla\"\n",
      "\n",
      "Score: 0.02000495232641697\t \n",
      "Topic: 0.340*\"yari\" + 0.220*\"toyota\" + 0.176*\"sell\" + 0.038*\"white\" + 0.038*\"black\" + 0.038*\"benz\" + 0.038*\"audi\" + 0.038*\"recondit\" + 0.038*\"green\" + 0.038*\"corolla\"\n",
      "\n",
      "Score: 0.0200020931661129\t \n",
      "Topic: 0.378*\"green\" + 0.165*\"recondit\" + 0.143*\"audi\" + 0.115*\"benz\" + 0.059*\"corolla\" + 0.042*\"toyota\" + 0.042*\"black\" + 0.018*\"sell\" + 0.018*\"white\" + 0.018*\"yari\"\n",
      "\n",
      "Score: 0.020002052187919617\t \n",
      "Topic: 0.403*\"yari\" + 0.260*\"toyota\" + 0.042*\"white\" + 0.042*\"audi\" + 0.042*\"benz\" + 0.042*\"black\" + 0.042*\"recondit\" + 0.042*\"sell\" + 0.042*\"green\" + 0.042*\"corolla\"\n",
      "\n",
      "Score: 0.020000042393803596\t \n",
      "Topic: 0.576*\"white\" + 0.223*\"benz\" + 0.025*\"recondit\" + 0.025*\"black\" + 0.025*\"audi\" + 0.025*\"sell\" + 0.025*\"green\" + 0.025*\"yari\" + 0.025*\"toyota\" + 0.025*\"corolla\"\n",
      "\n",
      "Score: 0.020000042393803596\t \n",
      "Topic: 0.334*\"audi\" + 0.334*\"black\" + 0.041*\"white\" + 0.041*\"benz\" + 0.041*\"recondit\" + 0.041*\"sell\" + 0.041*\"toyota\" + 0.041*\"yari\" + 0.041*\"green\" + 0.041*\"corolla\"\n",
      "\n",
      "Score: 0.020000042393803596\t \n",
      "Topic: 0.595*\"audi\" + 0.045*\"sell\" + 0.045*\"white\" + 0.045*\"black\" + 0.045*\"benz\" + 0.045*\"recondit\" + 0.045*\"green\" + 0.045*\"toyota\" + 0.045*\"yari\" + 0.045*\"corolla\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check which topic our test document belongs to using the LDA TF-IDF model.\n",
    "'''\n",
    "# Our test document is document number 4310\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c297cd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6999733448028564\t Topic: 0.290*\"black\" + 0.286*\"toyota\" + 0.228*\"corolla\" + 0.051*\"sell\" + 0.049*\"benz\"\n",
      "Score: 0.033344052731990814\t Topic: 0.241*\"sell\" + 0.234*\"toyota\" + 0.224*\"recondit\" + 0.211*\"corolla\" + 0.018*\"audi\"\n",
      "Score: 0.03333962708711624\t Topic: 0.291*\"sell\" + 0.260*\"toyota\" + 0.206*\"yari\" + 0.057*\"recondit\" + 0.030*\"benz\"\n",
      "Score: 0.033338434994220734\t Topic: 0.237*\"toyota\" + 0.220*\"yari\" + 0.195*\"carb\" + 0.104*\"recondit\" + 0.073*\"audi\"\n",
      "Score: 0.033336542546749115\t Topic: 0.230*\"white\" + 0.191*\"sell\" + 0.176*\"recondit\" + 0.150*\"toyota\" + 0.148*\"allion\"\n",
      "Score: 0.03333407640457153\t Topic: 0.423*\"black\" + 0.321*\"sell\" + 0.115*\"audi\" + 0.049*\"white\" + 0.027*\"recondit\"\n",
      "Score: 0.03333362936973572\t Topic: 0.395*\"recondit\" + 0.262*\"green\" + 0.161*\"benz\" + 0.121*\"audi\" + 0.025*\"sell\"\n",
      "Score: 0.033333566039800644\t Topic: 0.420*\"benz\" + 0.169*\"black\" + 0.165*\"sell\" + 0.147*\"recondit\" + 0.028*\"white\"\n",
      "Score: 0.03333338722586632\t Topic: 0.530*\"white\" + 0.182*\"audi\" + 0.139*\"benz\" + 0.129*\"recondit\" + 0.010*\"black\"\n",
      "Score: 0.03333335369825363\t Topic: 0.469*\"audi\" + 0.199*\"sell\" + 0.197*\"green\" + 0.100*\"black\" + 0.016*\"recondit\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"I want to buy a toyota corolla car in 2008\"\n",
    "\n",
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56a7b22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brandnew has no Synonyms in the API\n",
      "which has no Synonyms in the API\n",
      "[{'want': ['thirst', 'yearn', 'begrudge', 'hunger', 'ambition', 'starve', 'take to', 'lust after', 'long', 'fancy', 'feel like', 'care', 'hope', 'crave', 'like', 'wish', 'lust', 'go for', 'itch', 'miss', 'wish well', 'seek', 'spoil', 'hanker', 'desire', 'envy', 'lech after']}, {'buy': ['impulse-buy', 'choose', 'pick out', 'take', 'subscribe to', 'buy out', 'take out', 'subscribe', 'buy up', 'get', 'pick up', 'acquire', 'buy food', 'repurchase', 'pay', 'take over', 'select', 'purchase', 'buy back']}, None, {'recondition': ['condition']}, {'sell': ['deaccession', 'foist off', 'sell off', 'syndicate', 'undercut', 'wholesale', 'realize', 'underprice', 'palm off', 'auction off', 'scalp', 'auctioneer', 'resell', 'move', 'sell short', 'negociate', 'dump', 'sacrifice', 'auction', 'prostitute', 'clear', 'bootleg', 'dispose', 'remainder', 'give', 'fob off', 'change', 'undersell', 'interchange', 'exchange', 'realise', 'retail']}, None, {'made': ['ready-made']}]\n"
     ]
    }
   ],
   "source": [
    "from PyDictionary import PyDictionary\n",
    "\n",
    "def get_synonyms(word):\n",
    "    \n",
    "    synonyms = set()\n",
    "    \n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            \n",
    "            dictionary=PyDictionaryset('sem.csv')\n",
    "\n",
    "#print(dictionary.printMeanings()) \n",
    "\n",
    "#print(dictionary.getMeanings()) \n",
    "print (dictionary.getSynonyms())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14e3bf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence => ['I want to buy brandnew toyota corolla black car which made in 2008'\n",
      " 'I want to buy brandnew bmw x1 white suv which made in 2007'\n",
      " 'I want to buy recondition benz c200 green car which made in 2017'\n",
      " 'I want to buy brandnew audi a1 ash car which made in 2007'\n",
      " 'I want to sell brandnew bmw x3 black suv which made in 2007'\n",
      " 'I want to buy recondition bmw x2 red suv which made in 2007'\n",
      " 'I want to sell recondition toyota allion white car which made in 2007'\n",
      " 'I want to sell brandnew benz c180 ash car which made in 2014'\n",
      " 'I want to buy brandnew audi a3 black car which made in 2007'\n",
      " 'I want to buy brandnew toyota yaris red carb which made in 2018'\n",
      " 'I want to sell brandnew audi a1 green car which made in 2007'\n",
      " 'I want to sell recondition benz c180 black car which made in 2012'\n",
      " 'I want to buy brandnew benz c200 white car which made in 2007'\n",
      " 'I want to buy recondition audi a3 green car which made in 2008'\n",
      " 'I want to sell recondition toyota corolla red car which made in 2008'\n",
      " 'I want to buy brandnew benz c180 ash car which made in 2012'\n",
      " 'I want to sell brandnew bmw x3 black suv which made in 2014'\n",
      " 'I want to buy recondition audi a3 white car which made in 2007'\n",
      " 'I want to sell brandnew toyota yaris red car which made in 2008'\n",
      " 'I want to buy brandnew toyota KDH black car which made in 2008'\n",
      " 'I want to buy recondition benz c200 green car which made in 2018'\n",
      " 'I want to buy brandnew audi a1 ash car which made in 2020'\n",
      " 'I want to sell brandnew benz c180 ash car which made in 2018'\n",
      " 'I want to buy brandnew toyota yaris red carb which made in 2019'\n",
      " 'I want to buy recondition benz c180 black car which made in 2018'\n",
      " 'I want to buy recondition audi a3 green car which made in 2011'\n",
      " 'I want to sell recondition toyota corolla red car which made in 2009'\n",
      " 'I want to buy brandnew benz c180 ash car which made in 2008'\n",
      " 'I want to sell brandnew bmw x3 black suv which made in 2008'\n",
      " 'I want to buy recondition audi a3 white car which made in 2017'\n",
      " 'I want to buy brandnew toyota corolla black car which made in 2012'\n",
      " 'I want to buy recondition benz c200 green car which made in 2008'\n",
      " 'I want to buy brandnew audi a1 ash car which made in 2013'\n",
      " 'I want to sell brandnew benz c180 ash car which made in 2021'\n",
      " 'I want to buy recondition audi a3 black car which made in 2012 '\n",
      " 'I want to buy brandnew toyota yaris red carb which made in 2008'\n",
      " 'I want to sell brandnew audi a1 green car which made in 2014'\n",
      " 'I want to buy recondition benz c180 black car which made in 2008'\n",
      " 'I want to buy brandnew benz c200 white car which made in 2016'\n",
      " 'I want to sell brandnew bmw x1 white suv which made in 2020'\n",
      " 'I want to buy brandnew audi a1 ash car which made in 2021'\n",
      " 'I want to sell brandnew bmw x3 black suv which made in 2017'\n",
      " 'I want to buy recondition bmw x2 red suv which made in 2019'\n",
      " 'I want to sell recondition toyota allion white car which made in 2019'\n",
      " 'I want to sell brandnew benz c180 ash car which made in 2008'\n",
      " 'I want to buy brandnew bmw x1 white suv which made in 2015'\n",
      " 'I want to buy brandnew benz c180 ash car which made in 2016'\n",
      " 'I want to buy recondition audi a3 white car which made in 2015'\n",
      " 'I want to buy brandnew toyota corolla black car which made in 2020'\n",
      " 'I want to buy brandnew bmw x1 white suv which made in 2009'\n",
      " 'I want to buy recondition benz c200 green car which made in 2010'\n",
      " 'I want to buy brandnew audi a1 ash car which made in 2010'\n",
      " 'I want to sell brandnew bmw x3 black suv which made in 2011'\n",
      " 'I want to buy recondition bmw x2 red suv which made in 2013'\n",
      " 'I want to sell recondition toyota allion white car which made in 2015'\n",
      " 'I want to sell brandnew benz c180 ash car which made in 2010'\n",
      " 'I want to buy brandnew audi a3 black car which made in 2011'\n",
      " 'I want to buy recondition audi a3 white car which made in 2014'\n",
      " 'I want to buy brandnew bmw x1 white suv which made in 2019'\n",
      " 'I want to sell brandnew bmw x3 black suv which made in 2010'\n",
      " 'I want to buy recondition bmw x2 red suv which made in 2011'\n",
      " 'I want to buy brandnew toyota yaris red carb which made in 2010'\n",
      " 'I want to buy recondition benz c180 black car which made in 2010'\n",
      " 'I want to buy recondition bmw x2 red suv which made in 2020'\n",
      " 'I want to sell recondition toyota allion white car which made in 2013'\n",
      " 'I want to buy brandnew bmw x1 white suv which made in 2018'\n",
      " 'I want to buy brandnew audi a3 black car which made in 2009'\n",
      " 'I want to buy recondition benz c180 black car which made in 2011'\n",
      " 'I want to sell recondition toyota corolla red car which made in 2019'\n",
      " 'I want to buy brandnew audi a1 ash car which made in 2017'\n",
      " 'I want to buy brandnew toyota yaris red carb which made in 2009'\n",
      " 'I want to buy recondition benz c180 black car which made in 2009'\n",
      " 'I want to sell brandnew bmw x3 black suv which made in 2016'\n",
      " 'I want to buy recondition bmw x2 red suv which made in 2016'\n",
      " 'I want to buy brandnew audi a3 black car which made in 2019'\n",
      " 'I want to buy brandnew benz c200 white car which made in 2020'\n",
      " 'I want to buy recondition audi a3 white car which made in 2010'\n",
      " 'I want to buy brandnew audi a1 ash car which made in 2015'\n",
      " 'I want to buy brandnew bmw x1 white suv which made in 2010'\n",
      " 'I want to buy brandnew audi a3 black car which made in 2010'\n",
      " 'I want to sell brandnew audi a1 green car which made in 2017'\n",
      " 'I want to buy brandnew benz c200 white car which made in 2018'\n",
      " 'I want to buy recondition audi a3 white car which made in 2020'\n",
      " 'I want to buy brandnew audi a1 ash car which made in 2012'\n",
      " 'I want to sell brandnew bmw x3 black suv which made in 2012'\n",
      " 'I want to buy recondition bmw x2 red suv which made in 2012'\n",
      " 'I want to sell recondition toyota allion white car which made in 2012'\n",
      " 'I want to buy brandnew audi a3 black car which made in 2015'\n",
      " 'I want to sell brandnew audi a1 green car which made in 2013'\n",
      " 'I want to buy brandnew benz c200 white car which made in 2013'\n",
      " 'I want to buy recondition audi a3 white car which made in 2011'\n",
      " 'I want to buy brandnew bmw x1 white suv which made in 2011'\n",
      " 'I want to sell brandnew bmw x3 black suv which made in 2009'\n",
      " 'I want to buy recondition bmw x2 red suv which made in 2009'\n",
      " 'I want to sell recondition toyota allion white car which made in 2011'\n",
      " 'I want to buy brandnew audi a3 black car which made in 2008'\n",
      " 'I want to sell recondition toyota allion white car which made in 2014'\n",
      " 'I want to buy brandnew benz c200 white car which made in 2017'\n",
      " 'I want to buy recondition bmw x2 red suv which made in 2015'\n",
      " 'I want to sell recondition toyota allion white car which made in 2009'\n",
      " 'I want to sell brandnew audi a1 green car which made in 2019'\n",
      " 'I want to buy brandnew benz c200 white car which made in 2009'\n",
      " 'I want to buy brandnew bmw x1 white suv which made in 2014'\n",
      " 'I want to buy brandnew audi a1 ash car which made in 2014'\n",
      " 'I want to sell brandnew bmw x3 black suv which made in 2013'\n",
      " 'I want to sell recondition toyota allion white car which made in 2016'\n",
      " 'I want to buy brandnew bmw x1 white suv which made in 2008'\n",
      " 'I want to buy brandnew audi a3 black car which made in 2018'\n",
      " 'I want to sell brandnew audi a1 green car which made in 2020'\n",
      " 'I want to buy recondition audi a3 white car which made in 2012'\n",
      " 'I want to buy brandnew bmw x1 white suv which made in 2016'\n",
      " 'I want to buy brandnew audi a1 ash car which made in 2018'\n",
      " 'I want to sell brandnew bmw x3 black suv which made in 2018'\n",
      " 'I want to buy brandnew benz c200 white car which made in 2019'\n",
      " 'I want to buy brandnew audi a1 ash car which made in 2019'\n",
      " 'I want to buy brandnew audi a3 black car which made in 2014'\n",
      " 'I want to sell brandnew benz c200 white car which made in 2007'\n",
      " 'I want to sell recondition audi a3 green car which made in 2008'\n",
      " 'I want to buy recondition bmw x2 red suv which made in 2018'\n",
      " 'I want to buy recondition audi a3 black car which made in 2011'\n",
      " 'I want to sell brandnew audi a1 green car which made in 2011'\n",
      " 'I want to buy brandnew benz c200 white car which made in 2015'\n",
      " 'I want to buy recondition audi a3 white car which made in 2018'\n",
      " 'I want to buy brandnew bmw x1 white suv which made in 2020'\n",
      " 'I want to buy recondition audi a3 white car which made in 2009'\n",
      " 'I want to sell brandnew bmw x1 white suv which made in 2007'\n",
      " 'I want to sell recondition toyota allion white car which made in 2017'\n",
      " 'I want to buy brandnew bmw x1 white suv which made in 2017'\n",
      " 'I want to buy recondition benz c200 green car which made in 2016'\n",
      " 'I want to sell brandnew benz c180 ash car which made in 2013'\n",
      " 'I want to buy brandnew toyota yaris red car which made in 2016'\n",
      " 'I want to buy recondition audi a3 green car which made in 2019'\n",
      " 'I want to sell recondition toyota corolla red car which made in 2014'\n",
      " 'I want to buy brandnew benz c180 ash car which made in 2019'\n",
      " 'I want to sell brandnew bmw x3 black suv which made in 2020'\n",
      " 'I want to sell brandnew toyota yaris red car which made in 2018'\n",
      " 'I want to buy brandnew toyota yaris red car which made in 2013'\n",
      " 'I want to sell recondition benz c180 black car which made in 2016'\n",
      " 'I want to sell recondition toyota KDH blue car which made in 2019'\n",
      " 'I want to buy brandnew audi a3 white car which made in 2007'\n",
      " 'I want to sell brandnew toyota yaris red car which made in 2009']\n",
      "Buy/Sell => ['buy' 'sell']\n",
      "Condition => ['brandnew' 'recondition']\n",
      "Brand => ['toyota' 'bmw' 'benz' 'audi']\n",
      "Model => ['corolla' 'x1' 'c200' 'a1' 'x3' 'x2' 'allion' 'c180' 'a3' 'yaris' 'KDH']\n",
      "Type => ['car' 'suv' 'van']\n",
      "Color => ['black' 'white' 'green' 'ash' 'red' 'blue']\n"
     ]
    }
   ],
   "source": [
    "#describe the unique values in categorical columns\n",
    "categorical_col = []\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == object:\n",
    "        categorical_col.append(col)\n",
    "        print(f\"{col} => {data[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8fd3c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#handling categorical variable through label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_col:\n",
    "    data[col] = label_encoder.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57a29cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Buy/Sell</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Model</th>\n",
       "      <th>Type</th>\n",
       "      <th>Color</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No  Sentence  Buy/Sell  Condition  Brand  Model  Type  Color  Year\n",
       "0   1        47         0          0      3      6     0      1  2008\n",
       "1   2        34         0          0      2      7     1      5  2007\n",
       "2   3        80         0          1      1      5     0      3  2017\n",
       "3   4         0         0          0      0      1     0      0  2007\n",
       "4   5       108         1          0      2      9     1      1  2007"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e5ca236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting dataset into dependent and independent variables\n",
    "X = data.drop(['No','Sentence'],axis=1)\n",
    "y = data.Sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "416a8e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#split dataset for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)#,random_state=42)\n",
    "\n",
    "model= DecisionTreeClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_test,predictions)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "687a2414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': LogisticRegression(),\n",
       " 'KNeighborsClassifier': KNeighborsClassifier(n_neighbors=2),\n",
       " 'SVC': SVC(random_state=42),\n",
       " 'DecisionTreeClassifier': DecisionTreeClassifier(random_state=10),\n",
       " 'RandomForestClassifier': RandomForestClassifier(n_estimators=60, random_state=0)}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "key = ['LogisticRegression','KNeighborsClassifier','SVC','DecisionTreeClassifier','RandomForestClassifier']\n",
    "value = [LogisticRegression(), KNeighborsClassifier(n_neighbors=2, weights = 'uniform'),SVC(kernel = \"rbf\", random_state=42),DecisionTreeClassifier(random_state=10), RandomForestClassifier(n_estimators=60, random_state=0)]\n",
    "models = dict(zip(key,value))\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7475f031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ushana\\anaconda3\\envs\\Semantic\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.52\n",
      "KNeighborsClassifier 0.73\n",
      "SVC 0.02\n",
      "DecisionTreeClassifier 0.75\n",
      "RandomForestClassifier 0.75\n"
     ]
    }
   ],
   "source": [
    "predicted =[]\n",
    "for name, algo in models.items():\n",
    "    model=algo\n",
    "    model.fit(X_train, y_train)\n",
    "    predict = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, predict)\n",
    "    predicted.append(acc)\n",
    "    print(name,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae13a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
